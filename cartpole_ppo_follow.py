# -*- coding: utf-8 -*-
"""cartpole_ppo_follow.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1MsRlEWRAk712AQPmoM9X9E6bNeHULRDb
"""

# import gym
# import matplotlib.pyplot as plt
import numpy as np
# import seaborn as sns
import torch
from torch import nn
from torch import optim
from torch.distributions.categorical import Categorical

# sns.set()

DEVICE = 'cpu'

# Policy and value model
# class ActorCriticNetwork(nn.Module):
#   def __init__(self, obs_space_size, action_space_size):
#     super().__init__()

#     self.shared_layers = nn.Sequential(
#         nn.Linear(obs_space_size, 64),
#         nn.ReLU(),
#         nn.Linear(64, 64),
#         nn.ReLU())

#     self.policy_layers = nn.Sequential(
#         nn.Linear(64, 64),
#         nn.ReLU(),
#         nn.Linear(64, action_space_size))

#     self.value_layers = nn.Sequential(
#         nn.Linear(64, 64),
#         nn.ReLU(),
#         nn.Linear(64, 1))

#   def value(self, obs):
#     z = self.shared_layers(obs)
#     value = self.value_layers(z)
#     return value

#   def policy(self, obs):
#     z = self.shared_layers(obs)
#     policy_logits = self.policy_layers(z)
#     return policy_logits

#   def forward(self, obs):
#     z = self.shared_layers(obs)
#     policy_logits = self.policy_layers(z)
#     value = self.value_layers(z)
#     return policy_logits, value

class PPOTrainer():
  def __init__(self,
              policy_network,
              critic_network,
              ppo_clip_val=0.2,
              target_kl_div=0.01,
              max_policy_train_iters=80,
              value_train_iters=80,
              policy_lr=3e-4,
              value_lr=1e-2):
    
    self.policy = policy_network
    self.critic = critic_network

    self.ppo_clip_val = ppo_clip_val
    self.target_kl_div = target_kl_div
    self.max_policy_train_iters = max_policy_train_iters
    self.value_train_iters = value_train_iters


    policy_params = self.policy.parameters()
    
    self.policy_optim = optim.Adam(policy_params, lr=policy_lr)

    value_params = self.critic.parameters()
    self.value_optim = optim.Adam(value_params, lr=value_lr)

  def train_policy(self, obs, acts, old_log_probs, gaes):
    for _ in range(self.max_policy_train_iters):
      self.policy_optim.zero_grad()

      obs_game_state = obs[0]
      obs_card_state = obs[1]

      new_logits = self.policy(obs_game_state,obs_card_state)
      new_logits = Categorical(logits=new_logits)

      # new_logits = self.ac.policy(obs)
      # new_logits = Categorical(logits=new_logits)
      # new_log_probs = new_logits.log_prob(acts)

      new_log_probs = new_logits.log_prob(acts)

      policy_ratio = torch.exp(new_log_probs - old_log_probs)
      clipped_ratio = policy_ratio.clamp(
          1 - self.ppo_clip_val, 1 + self.ppo_clip_val)

      clipped_loss = clipped_ratio * gaes
      full_loss = policy_ratio * gaes
      
      surr3 = 3*gaes


      #Pytorch can't comprehend taking the min of three arguments, so we do this
      min_full_clipped = torch.min(full_loss, clipped_loss)
      min_final = torch.min(min_full_clipped, surr3)
      policy_loss = -min_final.mean()
      
      # policy_loss = -torch.min(full_loss, clipped_loss,surr3).mean()
      
      policy_loss.backward()
      self.policy_optim.step()

      kl_div = (old_log_probs - new_log_probs).mean()
      if kl_div >= self.target_kl_div:
        break

  def train_value(self, obs, returns, ownchips, opponentchips):
    for _ in range(self.value_train_iters):
      self.value_optim.zero_grad()
      clampreturn = torch.clamp(returns, -ownchips, opponentchips)

      values = self.ac.value(obs)
      value_loss = (clampreturn - values) ** 2
      value_loss = value_loss.mean()

      value_loss.backward()
      self.value_optim.step()

def discount_rewards(rewards, gamma=0.999):
    """
    Return discounted rewards based on the given rewards and gamma param.
    """
    new_rewards = [float(rewards[-1])]
    for i in reversed(range(len(rewards)-1)):
        new_rewards.append(float(rewards[i]) + gamma * new_rewards[-1])
    return np.array(new_rewards[::-1])

def calculate_gae(reward, values, gamma=0.999, decay=0.97):
    """
    Return a list of General Advantage Estimates for each value in 'values', calculated using the given single reward.
    Each GAE is calculated using the formula and approach from the original paper (referenced in the docstring).
    """
    next_values = np.concatenate([values[1:], [0]])  # Prepare the next values array by appending a zero for the terminal state
    deltas = reward + gamma * next_values - values  # Calculate deltas for each state value

    gaes = np.zeros_like(values)  # Initialize GAEs array with zeros
    gae = 0.0  # Start with a GAE of zero
    for i in reversed(range(len(values))):  # Reverse iteration over values
        gae = deltas[i] + gamma * decay * gae  # Update GAE recursively
        gaes[i] = gae  # Store the computed GAE for the current index

    return gaes  # Return the array of GAEs

def rollout(model, env, max_steps=1000):
    """
    Performs a single rollout.
    Returns training data in the shape (n_steps, observation_shape)
    and the cumulative reward.
    """
    ### Create data storage
    train_data = [[], [], [], [], []] # obs, act, reward, values, act_log_probs
    obs = env.reset()

    ep_reward = 0
    for _ in range(max_steps):
        logits, val = model(torch.tensor([obs], dtype=torch.float32,
                                         device=DEVICE))
        act_distribution = Categorical(logits=logits)
        act = act_distribution.sample()
        act_log_prob = act_distribution.log_prob(act).item()

        act, val = act.item(), val.item()

        next_obs, reward, done, _ = env.step(act)

        for i, item in enumerate((obs, act, reward, val, act_log_prob)):
          train_data[i].append(item)

        obs = next_obs
        ep_reward += reward
        if done:
            break

#     train_data = [np.asarray(x) for x in train_data]

#     ### Do train data filtering
#     train_data[3] = calculate_gaes(train_data[2], train_data[3])

#     return train_data, ep_reward

# env = gym.make('CartPole-v0')
# model = ActorCriticNetwork(env.observation_space.shape[0], env.action_space.n)
# model = model.to(DEVICE)
# train_data, reward = rollout(model, env) # Test rollout function

# # Define training params
# n_episodes = 200
# print_freq = 20

# ppo = PPOTrainer(
#     model,
#     policy_lr = 3e-4,
#     value_lr = 1e-3,
#     target_kl_div = 0.02,
#     max_policy_train_iters = 40,
#     value_train_iters = 40)

# # Training loop
# ep_rewards = []
# def gay():
#   for episode_idx in range(n_episodes):
#     # Perform rollout
#     train_data, reward = rollout(model, env)
#     ep_rewards.append(reward)

#     # Shuffle
#     permute_idxs = np.random.permutation(len(train_data[0]))

#     # Policy data
#     obs = torch.tensor(train_data[0][permute_idxs],
#                       dtype=torch.float32, device=DEVICE)
    
#     acts = torch.tensor(train_data[1][permute_idxs],
#                         dtype=torch.int32, device=DEVICE)
    
#     gaes = torch.tensor(train_data[3][permute_idxs],
#                         dtype=torch.float32, device=DEVICE)
    
#     act_log_probs = torch.tensor(train_data[4][permute_idxs],
#                                 dtype=torch.float32, device=DEVICE)

#     #Value data
#     returns = discount_rewards(train_data[2])[permute_idxs]
#     returns = torch.tensor(returns, dtype=torch.float32, device=DEVICE)

#     # Train model
#     ppo.train_policy(obs, acts, act_log_probs, gaes)
#     ppo.train_value(obs, returns)

#     if (episode_idx + 1) % print_freq == 0:
#       print('Episode {} | Avg Reward {:.1f}'.format(
#           episode_idx + 1, np.mean(ep_rewards[-print_freq:])))

